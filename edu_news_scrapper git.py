import requests
from bs4 import BeautifulSoup
import json
import time
import re
from datetime import datetime
from urllib.parse import urljoin, urlparse

class EduNewsScraper:
    def __init__(self):
        self.base_url = "https://www.edu.tw"
        self.news_url = "https://www.edu.tw/News.aspx?n=9E7AC85F1954DDA8"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
    def get_news_list(self, page=1):
        """ç²å–æ–°èåˆ—è¡¨"""
        try:
            params = {
                'n': '9E7AC85F1954DDA8',
                'page': page
            }
            response = self.session.get(self.news_url, params=params)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.content, 'html.parser')
            return soup
        except Exception as e:
            print(f"ç²å–æ–°èåˆ—è¡¨å¤±æ•—: {e}")
            return None
    
    def extract_news_items(self, soup):
        """å¾é é¢æå–æ–°èé …ç›®"""
        news_items = []
        
        # å°‹æ‰¾æ–°èé …ç›®å®¹å™¨ï¼ˆéœ€è¦æ ¹æ“šå¯¦éš›ç¶²é çµæ§‹èª¿æ•´ï¼‰
        news_containers = soup.find_all('div', class_='news_item') or \
                         soup.find_all('tr', class_='news_row') or \
                         soup.find_all('li', class_='news') or \
                         soup.select('div.CP_list tr')
        
        if not news_containers:
            # å¦‚æœæ‰¾ä¸åˆ°ç‰¹å®šclassï¼Œå˜—è©¦å…¶ä»–é¸æ“‡å™¨
            news_containers = soup.select('table tr')[1:]  # è·³éè¡¨é ­
        
        for container in news_containers:
            try:
                # æå–æ¨™é¡Œå’Œé€£çµ
                title_elem = container.find('a') or container.find('td', string=re.compile(r'.+'))
                if not title_elem:
                    continue
                    
                title = title_elem.get_text(strip=True) if title_elem else ""
                link = ""
                
                if title_elem.name == 'a':
                    link = urljoin(self.base_url, title_elem.get('href', ''))
                
                # æå–æ™‚é–“
                date_text = ""
                date_elem = container.find('td', string=re.compile(r'\d{4}')) or \
                           container.find(string=re.compile(r'\d{4}'))
                if date_elem:
                    date_text = date_elem.strip()
                
                # æå–ç™¼å¸ƒå–®ä½
                unit = ""
                unit_elem = container.find('td')
                if unit_elem:
                    unit_text = unit_elem.get_text(strip=True)
                    # ç°¡å–®åˆ¤æ–·æ˜¯å¦ç‚ºç™¼å¸ƒå–®ä½
                    if any(keyword in unit_text for keyword in ['ç½²', 'å¸', 'å±€', 'è™•', 'éƒ¨', 'æœƒ']):
                        unit = unit_text
                
                if title and link:
                    news_items.append({
                        'title': title,
                        'link': link,
                        'date': date_text,
                        'unit': unit
                    })
                    
            except Exception as e:
                print(f"è§£ææ–°èé …ç›®æ™‚å‡ºéŒ¯: {e}")
                continue
        
        return news_items
    
    def get_news_detail(self, news_url):
        """ç²å–æ–°èè©³ç´°å…§å®¹"""
        try:
            response = self.session.get(news_url)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æå–å…§æ–‡
            content_selectors = [
                'div.content',
                'div.news-content', 
                'div.article-content',
                'div#content',
                'div.CP_content'
            ]
            
            content = ""
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    content = content_elem.get_text(strip=True)
                    break
            
            # æå–è¯çµ¡äººå’Œé›»è©±
            contact_info = self.extract_contact_info(content, soup)
            
            return {
                'contact_name': contact_info.get('person', ''),
                'contact_tel': contact_info.get('phone', '')
            }
            
        except Exception as e:
            print(f"ç²å–æ–°èè©³æƒ…å¤±æ•— {news_url}: {e}")
            return {
                'contact_name': '',
                'contact_tel': ''
            }
    
    def extract_contact_info(self, content, soup):
        """æå–è¯çµ¡äººå’Œé›»è©±ä¿¡æ¯"""
        contact_info = {'person': '', 'phone': ''}
        
        # å¸¸è¦‹çš„è¯çµ¡äººæ¨¡å¼
        contact_patterns = [
            r'è¯çµ¡äºº[ï¼š:]\s*([^\s\n]+)',
            r'é€£çµ¡äºº[ï¼š:]\s*([^\s\n]+)',
            r'æ–°èè¯çµ¡äºº[ï¼š:]\s*([^\s\n]+)',
            r'æ‰¿è¾¦äºº[ï¼š:]\s*([^\s\n]+)'
        ]
        
        # é›»è©±è™Ÿç¢¼æ¨¡å¼
        phone_patterns = [
            r'é›»è©±[ï¼š:]?\s*(\(?0\d{1,2}\)?\s*-?\s*\d{3,4}\s*-?\s*\d{4})',
            r'è¯çµ¡é›»è©±[ï¼š:]?\s*(\(?0\d{1,2}\)?\s*-?\s*\d{3,4}\s*-?\s*\d{4})',
            r'(?:TEL|Tel|tel)[ï¼š:]?\s*(\(?0\d{1,2}\)?\s*-?\s*\d{3,4}\s*-?\s*\d{4})'
        ]
        
        # æœå°‹è¯çµ¡äºº
        for pattern in contact_patterns:
            match = re.search(pattern, content)
            if match:
                contact_info['person'] = match.group(1).strip()
                break
        
        # æœå°‹é›»è©±
        for pattern in phone_patterns:
            match = re.search(pattern, content)
            if match:
                contact_info['phone'] = match.group(1).strip()
                break
        
        return contact_info
    
    def filter_by_unit(self, news_items, target_unit):
        """æ ¹æ“šç™¼å¸ƒå–®ä½éæ¿¾æ–°è"""
        filtered_items = []
        for item in news_items:
            if target_unit in item.get('unit', '') or target_unit in item.get('title', ''):
                filtered_items.append(item)
        return filtered_items
    
    def scrape_news(self, target_unit, max_count):
        """ä¸»è¦çˆ¬å–å‡½æ•¸"""
        print(f"é–‹å§‹çˆ¬å– {target_unit} çš„æ–°èï¼Œç›®æ¨™æ•¸é‡: {max_count}")
        
        scraped_news = []
        page = 1
        max_pages = 10  # é˜²æ­¢ç„¡é™å¾ªç’°
        
        while len(scraped_news) < max_count and page <= max_pages:
            print(f"æ­£åœ¨çˆ¬å–ç¬¬ {page} é ...")
            
            soup = self.get_news_list(page)
            if not soup:
                print(f"ç„¡æ³•ç²å–ç¬¬ {page} é å…§å®¹")
                break
            
            news_items = self.extract_news_items(soup)
            if not news_items:
                print(f"ç¬¬ {page} é æ²’æœ‰æ‰¾åˆ°æ–°èé …ç›®")
                break
            
            # éæ¿¾æŒ‡å®šå–®ä½çš„æ–°è
            filtered_items = self.filter_by_unit(news_items, target_unit)
            
            for item in filtered_items:
                if len(scraped_news) >= max_count:
                    break
                
                print(f"è™•ç†æ–°è: {item['title'][:30]}...")
                
                # ç²å–è©³ç´°å…§å®¹
                detail = self.get_news_detail(item['link'])
                
                news_data = {
                    'date': item['date'],
                    'unit': item['unit'] or target_unit,
                    'title': item['title'],
                    'url': item['link'],
                    'author': {
                        'name': detail['contact_name'],
                        'tel': detail['contact_tel']
                    }
                }
                
                scraped_news.append(news_data)
                time.sleep(1)  # é¿å…éæ–¼é »ç¹çš„è«‹æ±‚
            
            page += 1
        
        print(f"æˆåŠŸçˆ¬å– {len(scraped_news)} ç­† {target_unit} çš„æ–°è")
        return scraped_news

def get_user_input():
    """ç²å–ç”¨æˆ¶è¼¸å…¥ä¸¦é€²è¡Œé˜²å‘†æª¢æŸ¥"""
    while True:
        try:
            # è¼¸å…¥æ–‡ç« æ•¸é‡
            count_input = input("è«‹è¼¸å…¥è¦çˆ¬å–çš„æ–‡ç« æ•¸é‡ (1-50): ").strip()
            if not count_input.isdigit():
                print("âŒ è«‹è¼¸å…¥æœ‰æ•ˆçš„æ•¸å­—ï¼")
                continue
            
            count = int(count_input)
            if count < 1 or count > 50:
                print("âŒ æ–‡ç« æ•¸é‡å¿…é ˆåœ¨ 1-50 ä¹‹é–“ï¼")
                continue
            
            # è¼¸å…¥ç™¼å¸ƒå–®ä½
            print("\nå¯é¸çš„ç™¼å¸ƒå–®ä½: é«”è‚²ç½²ã€åœ‹æ•™ç½²ã€é’å¹´ç½²ã€é«˜æ•™å¸ã€æŠ€è·å¸ç­‰")
            unit_input = input("è«‹è¼¸å…¥è¦çˆ¬å–çš„ç™¼å¸ƒå–®ä½: ").strip()
            if not unit_input:
                print("âŒ ç™¼å¸ƒå–®ä½ä¸èƒ½ç‚ºç©ºï¼")
                continue
            
            # ç¢ºèªè¼¸å…¥
            print(f"\nâœ… ç¢ºèªè¨­å®š:")
            print(f"   æ–‡ç« æ•¸é‡: {count}")
            print(f"   ç™¼å¸ƒå–®ä½: {unit_input}")
            
            confirm = input("ç¢ºèªä»¥ä¸Šè¨­å®šå—ï¼Ÿ(y/n): ").strip().lower()
            if confirm in ['y', 'yes', 'æ˜¯']:
                return count, unit_input
            else:
                print("è«‹é‡æ–°è¼¸å…¥...\n")
                
        except KeyboardInterrupt:
            print("\nç¨‹å¼å·²å–æ¶ˆåŸ·è¡Œ")
            exit()
        except Exception as e:
            print(f"âŒ è¼¸å…¥éŒ¯èª¤: {e}")

def save_to_json(data, filename):
    """ä¿å­˜æ•¸æ“šåˆ°JSONæ–‡ä»¶"""
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"âœ… æ•¸æ“šå·²ä¿å­˜åˆ° {filename}")
    except Exception as e:
        print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±æ•—: {e}")

def main():
    """ä¸»å‡½æ•¸"""
    print("=" * 50)
    print("ğŸ« æ•™è‚²éƒ¨æ–°èçˆ¬èŸ²ç¨‹å¼")
    print("=" * 50)
    
    scraper = EduNewsScraper()
    
    # æ–¹å¼ä¸€ï¼šæ‰‹å‹•è¼¸å…¥æ¨¡å¼
    mode = input("é¸æ“‡æ¨¡å¼ - 1: æ‰‹å‹•è¼¸å…¥, 2: é è¨­çˆ¬å–é«”è‚²ç½²/åœ‹æ•™ç½²/é’å¹´ç½²å„5ç­† (1/2): ").strip()
    
    if mode == "1":
        # æ‰‹å‹•è¼¸å…¥æ¨¡å¼
        count, unit = get_user_input()
        
        print(f"\nğŸš€ é–‹å§‹çˆ¬å–...")
        news_data = scraper.scrape_news(unit, count)
        
        if news_data:
            filename = f"{unit}_news_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            save_to_json(news_data, filename)
        else:
            print("âŒ æ²’æœ‰çˆ¬å–åˆ°ä»»ä½•æ•¸æ“š")
    
    elif mode == "2":
        # é è¨­æ¨¡å¼ï¼šçˆ¬å–é«”è‚²ç½²ã€åœ‹æ•™ç½²ã€é’å¹´ç½²å„5ç­†
        units = ['é«”è‚²ç½²', 'åœ‹æ•™ç½²', 'é’å¹´ç½²']
        all_results = {}
        
        for unit in units:
            print(f"\nğŸš€ é–‹å§‹çˆ¬å– {unit} çš„æ–°è...")
            news_data = scraper.scrape_news(unit, 5)
            all_results[unit] = news_data
            time.sleep(2)  # å–®ä½é–“æš«åœ
        
        # ä¿å­˜åˆä½µçµæœ
        result_data = []
        for unit, data in all_results.items():
            result_data.extend(data)
        
        filename = f"edu_news_combined_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        save_to_json(result_data, filename)
        
        # é¡¯ç¤ºæ‘˜è¦
        print("\nğŸ“Š çˆ¬å–æ‘˜è¦:")
        for unit, data in all_results.items():
            print(f"   {unit}: {len(data)} ç¯‡æ–‡ç« ")
    
    else:
        print("âŒ ç„¡æ•ˆçš„é¸æ“‡")

if __name__ == "__main__":
    main()