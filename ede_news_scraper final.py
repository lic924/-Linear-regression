import requests
from bs4 import BeautifulSoup
import json
import time
import re
from datetime import datetime
from urllib.parse import urljoin, urlparse

class EduNewsScraper:
    def __init__(self):
        self.base_url = "https://www.edu.tw"
        # ä¿®æ­£ç¶²å€ï¼ŒåŠ å…¥ç¼ºå°‘çš„ sms åƒæ•¸
        self.news_url = "https://www.edu.tw/News.aspx"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
    def get_news_list(self, page=1):
        """ç²å–æ–°èåˆ—è¡¨"""
        try:
            # ä¿®æ­£åƒæ•¸
            params = {
                'n': '9E7AC85F1954DDA8',
                'sms': '169B8E91BB75571F',
                'page': page
            }
            
            print(f"æ­£åœ¨è«‹æ±‚ç¶²å€: {self.news_url} åƒæ•¸: {params}")
            response = self.session.get(self.news_url, params=params, timeout=30)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.content, 'html.parser')
            print(f"é é¢æ¨™é¡Œ: {soup.title.string if soup.title else 'ç„¡æ³•å–å¾—æ¨™é¡Œ'}")
            return soup
            
        except requests.exceptions.RequestException as e:
            print(f"ç¶²è·¯è«‹æ±‚éŒ¯èª¤: {e}")
            return None
        except Exception as e:
            print(f"ç²å–æ–°èåˆ—è¡¨å¤±æ•—: {e}")
            return None
    
    def extract_news_items(self, soup):
        """å¾é é¢æå–æ–°èé …ç›®"""
        news_items = []
        
        # æ ¹æ“šå¯¦éš›ç¶²é çµæ§‹ï¼Œå˜—è©¦å¤šç¨®é¸æ“‡å™¨
        selectors_to_try = [
            # è¡¨æ ¼è¡Œé¸æ“‡å™¨ï¼ˆæœ€å¸¸è¦‹ï¼‰
            'table tr',
            'tbody tr',
            # åˆ—è¡¨é¸æ“‡å™¨
            'div.news_item',
            'li.news',
            '.CP_list tr',
            # é€šç”¨é¸æ“‡å™¨
            'tr:has(a)',
            'div:has(a[href*="News_Content"])'
        ]
        
        news_containers = []
        for selector in selectors_to_try:
            try:
                containers = soup.select(selector)
                if containers and len(containers) > 1:  # è‡³å°‘è¦æœ‰2å€‹ä»¥ä¸Šæ‰ç®—æœ‰æ•ˆ
                    news_containers = containers[1:] if selector.endswith('tr') else containers  # è·³éè¡¨é ­
                    print(f"ä½¿ç”¨é¸æ“‡å™¨: {selector}, æ‰¾åˆ° {len(news_containers)} å€‹é …ç›®")
                    break
            except:
                continue
        
        if not news_containers:
            print("âŒ ç„¡æ³•æ‰¾åˆ°æ–°èé …ç›®å®¹å™¨ï¼Œå˜—è©¦å°å‡ºé é¢çµæ§‹...")
            # è¼¸å‡ºéƒ¨åˆ†é é¢çµæ§‹ä»¥ä¾¿èª¿è©¦
            print("é é¢å…§å®¹é è¦½:")
            print(str(soup)[:2000] + "..." if len(str(soup)) > 2000 else str(soup))
            return []
        
        print(f"é–‹å§‹è§£æ {len(news_containers)} å€‹æ–°èé …ç›®...")
        
        for i, container in enumerate(news_containers):
            try:
                # å°‹æ‰¾æ–°èé€£çµ
                link_elem = container.find('a', href=re.compile(r'News_Content|news|News'))
                if not link_elem:
                    # å˜—è©¦æ‰¾ä»»ä½•é€£çµ
                    link_elem = container.find('a')
                
                if not link_elem:
                    continue
                
                # æå–æ¨™é¡Œ
                title = link_elem.get_text(strip=True)
                if not title or len(title) < 3:  # æ¨™é¡Œå¤ªçŸ­å¯èƒ½ä¸æ˜¯æ–°è
                    continue
                
                # æå–é€£çµ
                href = link_elem.get('href', '')
                if href:
                    link = urljoin(self.base_url, href)
                else:
                    continue
                
                # æå–æ™‚é–“ï¼ˆå°‹æ‰¾åŒ…å«æ—¥æœŸæ ¼å¼çš„æ–‡å­—ï¼‰
                date_text = ""
                all_text = container.get_text()
                date_patterns = [
                    r'(\d{3}-\d{2}-\d{2})',  # 114-08-14
                    r'(\d{4}-\d{2}-\d{2})',  # 2025-08-14
                    r'(\d{1,2}/\d{1,2}/\d{4})',  # 8/14/2025
                ]
                
                for pattern in date_patterns:
                    match = re.search(pattern, all_text)
                    if match:
                        date_text = match.group(1)
                        break
                
                # æå–ç™¼å¸ƒå–®ä½ï¼ˆé€šå¸¸åœ¨è¡¨æ ¼çš„æŸä¸€æ¬„ï¼‰
                unit = ""
                td_elements = container.find_all('td')
                for td in td_elements:
                    td_text = td.get_text(strip=True)
                    # æª¢æŸ¥æ˜¯å¦åŒ…å«å¸¸è¦‹çš„æ•™è‚²éƒ¨å–®ä½åç¨±
                    unit_keywords = ['ç½²', 'å¸', 'è™•', 'éƒ¨', 'æœƒ', 'å±€', 'ä¸­å¿ƒ']
                    if any(keyword in td_text for keyword in unit_keywords) and len(td_text) < 20:
                        unit = td_text
                        break
                
                news_item = {
                    'title': title,
                    'link': link,
                    'date': date_text,
                    'unit': unit
                }
                
                news_items.append(news_item)
                print(f"  {i+1}. {title[:50]}... [{unit}] [{date_text}]")
                
            except Exception as e:
                print(f"è§£æç¬¬ {i+1} å€‹é …ç›®æ™‚å‡ºéŒ¯: {e}")
                continue
        
        print(f"æˆåŠŸæå– {len(news_items)} å€‹æ–°èé …ç›®")
        return news_items
    
    def get_news_detail(self, news_url):
        """ç²å–æ–°èè©³ç´°å…§å®¹"""
        try:
            print(f"æ­£åœ¨ç²å–è©³ç´°å…§å®¹: {news_url}")
            response = self.session.get(news_url, timeout=30)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æå–å…§æ–‡ï¼ˆå˜—è©¦å¤šç¨®é¸æ“‡å™¨ï¼‰
            content_selectors = [
                'div.content',
                'div.news-content', 
                'div.article-content',
                'div#content',
                'div.CP_content',
                '.page_content',
                'div:contains("è¯çµ¡äºº")',
                'div:contains("é›»è©±")'
            ]
            
            content = ""
            for selector in content_selectors:
                try:
                    if selector.startswith('div:contains'):
                        # ç‰¹æ®Šè™•ç†åŒ…å«æ–‡å­—çš„é¸æ“‡å™¨
                        elements = soup.find_all('div')
                        for elem in elements:
                            if 'è¯çµ¡äºº' in elem.get_text() or 'é›»è©±' in elem.get_text():
                                content = elem.get_text(strip=True)
                                break
                        if content:
                            break
                    else:
                        content_elem = soup.select_one(selector)
                        if content_elem:
                            content = content_elem.get_text(strip=True)
                            break
                except:
                    continue
            
            # å¦‚æœé‚„æ˜¯æ‰¾ä¸åˆ°ï¼Œå°±ç”¨æ•´å€‹é é¢å…§å®¹
            if not content:
                content = soup.get_text()
            
            # æå–è¯çµ¡äººå’Œé›»è©±
            contact_info = self.extract_contact_info(content, soup)
            
            return {
                'contact_name': contact_info.get('person', ''),
                'contact_tel': contact_info.get('phone', '')
            }
            
        except Exception as e:
            print(f"ç²å–æ–°èè©³æƒ…å¤±æ•— {news_url}: {e}")
            return {
                'contact_name': '',
                'contact_tel': ''
            }
    
    def extract_contact_info(self, content, soup):
        """æå–è¯çµ¡äººå’Œé›»è©±ä¿¡æ¯"""
        contact_info = {'person': '', 'phone': ''}
        
        # å¸¸è¦‹çš„è¯çµ¡äººæ¨¡å¼
        contact_patterns = [
            r'è¯çµ¡äºº[ï¼š:]\s*([^\s\n,ï¼Œï¼›;]+)',
            r'é€£çµ¡äºº[ï¼š:]\s*([^\s\n,ï¼Œï¼›;]+)',
            r'æ–°èè¯çµ¡äºº[ï¼š:]\s*([^\s\n,ï¼Œï¼›;]+)',
            r'æ‰¿è¾¦äºº[ï¼š:]\s*([^\s\n,ï¼Œï¼›;]+)',
            r'è¯çµ¡äºº\s*([^\s\n,ï¼Œï¼›;]+)',
        ]
        
        # é›»è©±è™Ÿç¢¼æ¨¡å¼ï¼ˆæ›´å®Œæ•´ï¼‰
        phone_patterns = [
            r'é›»è©±[ï¼š:]?\s*(\(?0\d{1,2}\)?\s*-?\s*\d{3,4}\s*-?\s*\d{3,4})',
            r'è¯çµ¡é›»è©±[ï¼š:]?\s*(\(?0\d{1,2}\)?\s*-?\s*\d{3,4}\s*-?\s*\d{3,4})',
            r'(?:TEL|Tel|tel)[ï¼š:]?\s*(\(?0\d{1,2}\)?\s*-?\s*\d{3,4}\s*-?\s*\d{3,4})',
            r'(\(?0\d{1,2}\)?\s*-?\s*\d{3,4}\s*-?\s*\d{3,4})',  # é€šç”¨é›»è©±æ ¼å¼
        ]
        
        # æœå°‹è¯çµ¡äºº
        for pattern in contact_patterns:
            match = re.search(pattern, content)
            if match:
                person_name = match.group(1).strip()
                # éæ¿¾å¤ªé•·çš„çµæœï¼ˆå¯èƒ½åŒ¹é…åˆ°éŒ¯èª¤å…§å®¹ï¼‰
                if len(person_name) <= 10 and not re.search(r'\d{3,}', person_name):
                    contact_info['person'] = person_name
                    break
        
        # æœå°‹é›»è©±
        for pattern in phone_patterns:
            match = re.search(pattern, content)
            if match:
                phone_number = match.group(1).strip()
                # ç°¡å–®é©—è­‰é›»è©±è™Ÿç¢¼æ ¼å¼
                if re.match(r'^\(?0\d{1,2}\)?[-\s]?\d{3,4}[-\s]?\d{3,4}$', phone_number):
                    contact_info['phone'] = phone_number
                    break
        
        return contact_info
    
    def filter_by_unit(self, news_items, target_unit):
        """æ ¹æ“šç™¼å¸ƒå–®ä½éæ¿¾æ–°è - åªæª¢æŸ¥å–®ä½æ¬„ä½ï¼Œä¸æª¢æŸ¥æ¨™é¡Œ"""
        filtered_items = []
        
        # å–®ä½é—œéµå­—æ˜ å°„
        unit_keywords = {
            'é«”è‚²ç½²': ['é«”è‚²ç½²', 'é«”è‚²', 'Sports'],
            'åœ‹æ•™ç½²': ['åœ‹æ•™ç½²', 'åœ‹æ°‘åŠå­¸å‰æ•™è‚²ç½²', 'åœ‹æ•™', 'K-12'],
            'é’å¹´ç½²': ['é’å¹´ç½²', 'é’å¹´ç™¼å±•ç½²', 'é’å¹´', 'Youth']
        }
        
        target_keywords = unit_keywords.get(target_unit, [target_unit])
        
        for item in news_items:
            unit_text = item.get('unit', '')
            
            # æª¢æŸ¥æ˜¯å¦åŒ¹é…ä»»ä¸€é—œéµå­— - åªæª¢æŸ¥å–®ä½
            match_found = False
            for keyword in target_keywords:
                if keyword in unit_text:  # åªæª¢æŸ¥ unit_text
                    match_found = True
                    break
            
            if match_found:
                filtered_items.append(item)
                
        return filtered_items
    
    def scrape_news(self, target_unit, max_count):
        """ä¸»è¦çˆ¬å–å‡½æ•¸ - æ”¹é€²ç‰ˆï¼šæŒçºŒçˆ¬å–ç›´åˆ°é”åˆ°ç›®æ¨™æ•¸é‡"""
        print(f"é–‹å§‹çˆ¬å– {target_unit} çš„æ–°èï¼Œç›®æ¨™æ•¸é‡: {max_count}")
        
        scraped_news = []
        page = 1
        max_pages = 100  # å¤§å¹…å¢åŠ æœ€å¤§é æ•¸
        consecutive_empty_pages = 0  # é€£çºŒç©ºé è¨ˆæ•¸
        max_empty_pages = 3  # é€£çºŒç©ºé ä¸Šé™
        
        while len(scraped_news) < max_count and page <= max_pages and consecutive_empty_pages < max_empty_pages:
            print(f"\n{'='*60}")
            print(f"æ­£åœ¨çˆ¬å–ç¬¬ {page} é ... (å·²æ”¶é›†: {len(scraped_news)}/{max_count})")
            print(f"{'='*60}")
            
            soup = self.get_news_list(page)
            if not soup:
                print(f"âŒ ç„¡æ³•ç²å–ç¬¬ {page} é å…§å®¹")
                consecutive_empty_pages += 1
                page += 1
                continue
            
            news_items = self.extract_news_items(soup)
            if not news_items:
                print(f"âŒ ç¬¬ {page} é æ²’æœ‰æ‰¾åˆ°æ–°èé …ç›®")
                consecutive_empty_pages += 1
                page += 1
                continue
            
            # é‡ç½®é€£çºŒç©ºé è¨ˆæ•¸
            consecutive_empty_pages = 0
            
            # éæ¿¾æŒ‡å®šå–®ä½çš„æ–°è
            filtered_items = self.filter_by_unit(news_items, target_unit)
            print(f"âœ… æ‰¾åˆ° {len(filtered_items)} ç¯‡ç¬¦åˆ '{target_unit}' çš„æ–°è")
            
            # å¦‚æœé€™ä¸€é æ²’æœ‰ç¬¦åˆçš„æ–°èï¼Œç¹¼çºŒä¸‹ä¸€é 
            if not filtered_items:
                print(f"âš ï¸ ç¬¬ {page} é æ²’æœ‰ {target_unit} çš„æ–°èï¼Œç¹¼çºŒæœå°‹...")
                page += 1
                time.sleep(1)
                continue
            
            # è™•ç†ç¬¦åˆæ¢ä»¶çš„æ–°è
            for item in filtered_items:
                if len(scraped_news) >= max_count:
                    break
                
                print(f"\nè™•ç†æ–°è {len(scraped_news)+1}/{max_count}: {item['title'][:50]}...")
                
                # æª¢æŸ¥æ˜¯å¦é‡è¤‡ï¼ˆæ ¹æ“šURLæˆ–æ¨™é¡Œï¼‰
                is_duplicate = False
                for existing_news in scraped_news:
                    if (existing_news['url'] == item['link'] or 
                        existing_news['title'] == item['title']):
                        print(f"âš ï¸ ç™¼ç¾é‡è¤‡æ–°èï¼Œè·³é: {item['title'][:30]}...")
                        is_duplicate = True
                        break
                
                if is_duplicate:
                    continue
                
                # ç²å–è©³ç´°å…§å®¹
                try:
                    detail = self.get_news_detail(item['link'])
                except Exception as e:
                    print(f"âš ï¸ ç„¡æ³•ç²å–è©³ç´°å…§å®¹: {e}")
                    detail = {'contact_name': '', 'contact_tel': ''}
                
                news_data = {
                    'date': item['date'],
                    'unit': item['unit'] or target_unit,
                    'title': item['title'],
                    'url': item['link'],
                    'author': {
                        'name': detail['contact_name'],
                        'tel': detail['contact_tel']
                    }
                }
                
                scraped_news.append(news_data)
                print(f"âœ… æˆåŠŸè™•ç†ç¬¬ {len(scraped_news)} ç¯‡æ–°è")
                
                # é©ç•¶å»¶é²é¿å…è¢«å°é–
                time.sleep(1)
            
            # å¦‚æœå·²ç¶“é”åˆ°ç›®æ¨™æ•¸é‡ï¼ŒçµæŸçˆ¬å–
            if len(scraped_news) >= max_count:
                print(f"\nğŸ‰ å·²é”åˆ°ç›®æ¨™æ•¸é‡ï¼")
                break
                
            page += 1
            time.sleep(1)  # é é¢é–“å»¶é²
            
            # æ¯10é é¡¯ç¤ºé€²åº¦
            if page % 10 == 0:
                print(f"\nğŸ“Š é€²åº¦å ±å‘Š: å·²æœå°‹ {page} é ï¼Œæ”¶é›†åˆ° {len(scraped_news)} ç¯‡æ–°è")
        
        # çµæœæ‘˜è¦
        if consecutive_empty_pages >= max_empty_pages:
            print(f"\nâš ï¸ é€£çºŒ {max_empty_pages} é æ²’æœ‰æ‰¾åˆ°å…§å®¹ï¼Œåœæ­¢æœå°‹")
        elif page > max_pages:
            print(f"\nâš ï¸ å·²é”åˆ°æœ€å¤§é æ•¸é™åˆ¶ ({max_pages} é )")
        
        print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼æˆåŠŸæ”¶é›† {len(scraped_news)} ç­† {target_unit} çš„æ–°è")
        return scraped_news

def get_user_input():
    """ç²å–ç”¨æˆ¶è¼¸å…¥ä¸¦é€²è¡Œé˜²å‘†æª¢æŸ¥"""
    while True:
        try:
            # è¼¸å…¥æ–‡ç« æ•¸é‡
            count_input = input("è«‹è¼¸å…¥è¦çˆ¬å–çš„æ–‡ç« æ•¸é‡ (1-80): ").strip()
            if not count_input.isdigit():
                print("âŒ è«‹è¼¸å…¥æœ‰æ•ˆçš„æ•¸å­—ï¼")
                continue
            
            count = int(count_input)
            if count < 1 or count > 80:  # æ¸›å°‘åˆ°20ç¯‡
                print("âŒ æ–‡ç« æ•¸é‡å¿…é ˆåœ¨ 1-80 ä¹‹é–“ï¼")
                continue
            
            # è¼¸å…¥ç™¼å¸ƒå–®ä½
            print("\nå¯é¸çš„ç™¼å¸ƒå–®ä½: é«”è‚²ç½²ã€åœ‹æ•™ç½²ã€é’å¹´ç½²")
            unit_input = input("è«‹è¼¸å…¥è¦çˆ¬å–çš„ç™¼å¸ƒå–®ä½: ").strip()
            if not unit_input:
                print("âŒ ç™¼å¸ƒå–®ä½ä¸èƒ½ç‚ºç©ºï¼")
                continue
            
            # ç¢ºèªè¼¸å…¥
            print(f"\nâœ… ç¢ºèªè¨­å®š:")
            print(f"   æ–‡ç« æ•¸é‡: {count}")
            print(f"   ç™¼å¸ƒå–®ä½: {unit_input}")
            
            confirm = input("ç¢ºèªä»¥ä¸Šè¨­å®šå—ï¼Ÿ(y/n): ").strip().lower()
            if confirm in ['y', 'yes', 'æ˜¯']:
                return count, unit_input
            else:
                print("è«‹é‡æ–°è¼¸å…¥...\n")
                
        except KeyboardInterrupt:
            print("\nç¨‹å¼å·²å–æ¶ˆåŸ·è¡Œ")
            exit()
        except Exception as e:
            print(f"âŒ è¼¸å…¥éŒ¯èª¤: {e}")

def save_to_json(data, filename):
    """ä¿å­˜æ•¸æ“šåˆ°JSONæ–‡ä»¶"""
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"âœ… æ•¸æ“šå·²ä¿å­˜åˆ° {filename}")
        return True
    except Exception as e:
        print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±æ•—: {e}")
        return False

def main():
    """ä¸»å‡½æ•¸"""
    print("=" * 50)
    print("ğŸ« æ•™è‚²éƒ¨æ–°èçˆ¬èŸ²ç¨‹å¼ (ä¿®æ­£ç‰ˆ)")
    print("=" * 50)
    
    scraper = EduNewsScraper()
    
    # æ–¹å¼ä¸€ï¼šæ‰‹å‹•è¼¸å…¥æ¨¡å¼
    mode = input("é¸æ“‡æ¨¡å¼ - 1: æ‰‹å‹•è¼¸å…¥, 2: æ¸¬è©¦æ¨¡å¼(çˆ¬å–é«”è‚²ç½²3ç¯‡) (1/2): ").strip()
    
    if mode == "1":
        # æ‰‹å‹•è¼¸å…¥æ¨¡å¼
        count, unit = get_user_input()
        
        print(f"\nğŸš€ é–‹å§‹çˆ¬å–...")
        news_data = scraper.scrape_news(unit, count)
        
        if news_data:
            filename = f"{unit}_news_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            if save_to_json(news_data, filename):
                print(f"\nğŸ“Š çˆ¬å–çµæœæ‘˜è¦:")
                print(f"   ç›®æ¨™å–®ä½: {unit}")
                print(f"   æˆåŠŸæ•¸é‡: {len(news_data)}")
                print(f"   å„²å­˜æª”æ¡ˆ: {filename}")
        else:
            print("âŒ æ²’æœ‰çˆ¬å–åˆ°ä»»ä½•æ•¸æ“šï¼Œè«‹æª¢æŸ¥ç¶²è·¯é€£ç·šæˆ–å–®ä½åç¨±")
    
    elif mode == "2":
        # æ¸¬è©¦æ¨¡å¼
        print(f"\nğŸ§ª æ¸¬è©¦æ¨¡å¼ï¼šçˆ¬å–é«”è‚²ç½² 3 ç¯‡æ–°è...")
        news_data = scraper.scrape_news("é«”è‚²ç½²", 3)
        
        if news_data:
            filename = f"test_sports_news_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            save_to_json(news_data, filename)
            print(f"\nğŸ“Š æ¸¬è©¦çµæœ: æˆåŠŸçˆ¬å– {len(news_data)} ç¯‡æ–°è")
        else:
            print("âŒ æ¸¬è©¦å¤±æ•—ï¼Œè«‹æª¢æŸ¥ç¶²è·¯é€£ç·š")
    
    else:
        print("âŒ ç„¡æ•ˆçš„é¸æ“‡")

if __name__ == "__main__":
    main()